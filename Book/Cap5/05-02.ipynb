{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d86f01-d8a0-4f38-909e-e7ebdb76df77",
   "metadata": {},
   "source": [
    "## Introducing Scikit-Learn\n",
    "Eber David Gaytan Medina\n",
    "\n",
    "There are several Python libraries which provide solid implementations of a range of machine learning algorithms. One of the best known is Scikit-Learn, a package that provides efficient versions of a large number of common algorithms. Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefit of this uniformity is that once you understand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.head()\n",
    "sepal_length\tsepal_width\tpetal_length\tpetal_width\tspecies\n",
    "0\t5.1\t3.5\t1.4\t0.2\tsetosa\n",
    "1\t4.9\t3.0\t1.4\t0.2\tsetosa\n",
    "2\t4.7\t3.2\t1.3\t0.2\tsetosa\n",
    "3\t4.6\t3.1\t1.5\t0.2\tsetosa\n",
    "4\t5.0\t3.6\t1.4\t0.2\tsetosa\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "sns.pairplot(iris, hue='species', size=1.5);\n",
    "\n",
    "\n",
    "X_iris = iris.drop('species', axis=1)\n",
    "X_iris.shape\n",
    "(150, 4)\n",
    "y_iris = iris['species']\n",
    "y_iris.shape\n",
    "(150,)\n",
    "T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 1 + rng.randn(50)\n",
    "plt.scatter(x, y);\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "\n",
    "X = x[:, np.newaxis]\n",
    "X.shape\n",
    "(50, 1)\n",
    "\n",
    "model.fit(X, y)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "\n",
    "model.coef_\n",
    "array([ 1.9776566])\n",
    "model.intercept_\n",
    "-0.90331072553111635\n",
    "\n",
    "xfit = np.linspace(-1, 11)\n",
    "As before, we need to coerce these x values into a [n_samples, n_features] features matrix, after which we can feed it to the model:\n",
    "\n",
    "Xfit = xfit[:, np.newaxis]\n",
    "yfit = model.predict(Xfit)\n",
    "Finally, let's visualize the results by plotting first the raw data, and then this model fit:\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,\n",
    "                                                random_state=1)\n",
    "With the data arranged, we can follow our recipe to predict the labels:\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
    "model = GaussianNB()                       # 2. instantiate model\n",
    "model.fit(Xtrain, ytrain)                  # 3. fit model to data\n",
    "y_model = model.predict(Xtest)             # 4. predict on new data\n",
    "Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)\n",
    "0.97368421052631582\n",
    "\n",
    "from sklearn.decomposition import PCA  # 1. Choose the model class\n",
    "model = PCA(n_components=2)            # 2. Instantiate the model with hyperparameters\n",
    "model.fit(X_iris)                      # 3. Fit to data. Notice y is not specified!\n",
    "X_2D = model.transform(X_iris)         # 4. Transform the data to two dimensions\n",
    "Now let's plot the results. A quick way to do this is to insert the results into the original Iris DataFrame, and use Seaborn's lmplot to show the results:\n",
    "\n",
    "iris['PCA1'] = X_2D[:, 0]\n",
    "iris['PCA2'] = X_2D[:, 1]\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", hue='species', data=iris, fit_reg=False);\n",
    "\n",
    "We see that in the two-dimensional representation, the species are fairly well separated, even though the PCA algorithm had no knowledge of the species labels! This indicates to us that a relatively straightforward classification will probably be effective on the dataset, as we saw before.\n",
    "\n",
    "Unsupervised learning: Iris clustering\n",
    "Let's next look at applying clustering to the Iris data. A clustering algorithm attempts to find distinct groups of data without reference to any labels. Here we will use a powerful clustering method called a Gaussian mixture model (GMM), discussed in more detail in In Depth: Gaussian Mixture Models. A GMM attempts to model the data as a collection of Gaussian blobs.\n",
    "\n",
    "We can fit the Gaussian mixture model as follows:\n",
    "\n",
    "from sklearn.mixture import GMM      # 1. Choose the model class\n",
    "model = GMM(n_components=3,\n",
    "            covariance_type='full')  # 2. Instantiate the model with hyperparameters\n",
    "model.fit(X_iris)                    # 3. Fit to data. Notice y is not specified!\n",
    "y_gmm = model.predict(X_iris)        # 4. Determine cluster labels\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.images.shape\n",
    "(1797, 8, 8)\n",
    "The images data is a three-dimensional array: 1,797 samples each consisting of an 8 Ã— 8 grid of pixels. Let's visualize the first hundred of these:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')\n",
    "\n",
    "In order to work with this data within Scikit-Learn, we need a two-dimensional, [n_samples, n_features] representation. We can accomplish this by treating each pixel in the image as a feature: that is, by flattening out the pixel arrays so that we have a length-64 array of pixel values representing each digit. Additionally, we need the target array, which gives the previously determined label for each digit. These two quantities are built into the digits dataset under the data and target attributes, respectively:\n",
    "\n",
    "X = digits.data\n",
    "X.shape\n",
    "(1797, 64)\n",
    "y = digits.target\n",
    "y.shape\n",
    "(1797,)\n",
    "\n",
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "iso.fit(digits.data)\n",
    "data_projected = iso.transform(digits.data)\n",
    "data_projected.shape\n",
    "(1797, 2)\n",
    "We see that the projected data is now two-dimensional. Let's plot this data to see if we can learn anything from its structure:\n",
    "\n",
    "plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,\n",
    "            edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('spectral', 10))\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "plt.clim(-0.5, 9.5);\n",
    "\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(Xtrain, ytrain)\n",
    "y_model = model.predict(Xtest)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, y_model)\n",
    "0.83333333333333337\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(ytest, y_model)\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value');\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "test_images = Xtest.reshape(-1, 8, 8)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(test_images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(y_model[i]),\n",
    "            transform=ax.transAxes,\n",
    "            color='green' if (ytest[i] == y_model[i]) else 'red')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

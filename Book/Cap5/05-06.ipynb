{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d86f01-d8a0-4f38-909e-e7ebdb76df77",
   "metadata": {},
   "source": [
    "## In Depth: Linear Regression\n",
    "Eber David Gaytan Medina\n",
    "\n",
    "Just as naive Bayes (discussed earlier in In Depth: Naive Bayes Classification) is a good starting point for classification tasks, linear regression models are a good starting point for regression tasks. Such models are popular because they can be fit very quickly, and are very interpretable. You are probably familiar with the simplest form of a linear regression model (i.e., fitting a straight line to data) but such models can be extended to model more complicated data behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "#Simple Linear Regression\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y);\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);\n",
    "\n",
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "Model slope:     2.02720881036\n",
    "Model intercept: -4.99857708555\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)\n",
    "0.5\n",
    "[ 1.5 -2.   1. ]\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])\n",
    "array([[  2.,   4.,   8.],\n",
    "       [  3.,   9.,  27.],\n",
    "       [  4.,  16.,  64.]])\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression())\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a54db6",
   "metadata": {},
   "source": [
    "\n",
    "Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!\n",
    "\n",
    "Gaussian basis functions\n",
    "Of course, other basis functions are possible. For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases. The result might look something like the following figure:\n",
    "\n",
    " figure source in Appendix\n",
    "\n",
    "The shaded regions in the plot are the scaled basis functions, and when added together they reproduce the smooth curve through the data. These Gaussian basis functions are not built into Scikit-Learn, but we can write a custom transformer that will create them, as shown here and illustrated in the following figure (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn's source is a good way to see how they can be created):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
    "                                 self.width_, axis=1)\n",
    "    \n",
    "gauss_model = make_pipeline(GaussianFeatures(20),\n",
    "                            LinearRegression())\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.xlim(0, 10);\n",
    "\n",
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5);\n",
    "\n",
    "\n",
    "def basis_plot(model, title=None):\n",
    "    fig, ax = plt.subplots(2, sharex=True)\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
    "    \n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    ax[1].plot(model.steps[0][1].centers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis location',\n",
    "              ylabel='coefficient',\n",
    "              xlim=(0, 10))\n",
    "    \n",
    "model = make_pipeline(GaussianFeatures(30), LinearRegression())\n",
    "basis_plot(model)\n",
    "\n",
    "Ridge regression (\n",
    " Regularization)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))\n",
    "basis_plot(model, title='Ridge Regression')\n",
    "\n",
    "Lasso regression (\n",
    " regularization)\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))\n",
    "basis_plot(model, title='Lasso Regression')\n",
    "\n",
    "import pandas as pd\n",
    "counts = pd.read_csv('FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "weather = pd.read_csv('data/BicycleWeather.csv', index_col='DATE', parse_dates=True)\n",
    "Next we will compute the total daily bicycle traffic, and put this in its own dataframe:\n",
    "\n",
    "daily = counts.resample('d').sum()\n",
    "daily['Total'] = daily.sum(axis=1)\n",
    "daily = daily[['Total']] # remove other columns\n",
    "We saw previously that the patterns of use generally vary from day to day; let's account for this in our data by adding binary columns that indicate the day of the week:\n",
    "\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for i in range(7):\n",
    "    daily[days[i]] = (daily.index.dayofweek == i).astype(float)\n",
    "Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays('2012', '2016')\n",
    "daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n",
    "daily['holiday'].fillna(0, inplace=True)\n",
    "We also might suspect that the hours of daylight would affect how many people ride; let's use the standard astronomical calculation to add this information:\n",
    "\n",
    "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
    "    days = (date - pd.datetime(2000, 12, 21)).days\n",
    "    m = (1. - np.tan(np.radians(latitude))\n",
    "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
    "\n",
    "daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n",
    "daily[['daylight_hrs']].plot()\n",
    "plt.ylim(8, 17)\n",
    "(8, 17)\n",
    "\n",
    "\n",
    "# temperatures are in 1/10 deg C; convert to C\n",
    "weather['TMIN'] /= 10\n",
    "weather['TMAX'] /= 10\n",
    "weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n",
    "\n",
    "# precip is in 1/10 mm; convert to inches\n",
    "weather['PRCP'] /= 254\n",
    "weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n",
    "\n",
    "daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])\n",
    "\n",
    "daily['annual'] = (daily.index - daily.index[0]).days / 365.\n",
    "\n",
    "daily.head()\n",
    "Total\tMon\tTue\tWed\tThu\tFri\tSat\tSun\tholiday\tdaylight_hrs\tPRCP\tTemp (C)\tdry day\tannual\n",
    "Date\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "2012-10-03\t3521.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t11.277359\t0.0\t13.35\t1.0\t0.000000\n",
    "2012-10-04\t3475.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t0.0\t11.219142\t0.0\t13.60\t1.0\t0.002740\n",
    "2012-10-05\t3148.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t0.0\t11.161038\t0.0\t15.30\t1.0\t0.005479\n",
    "2012-10-06\t2006.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t11.103056\t0.0\t15.85\t1.0\t0.008219\n",
    "2012-10-07\t2142.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t11.045208\t0.0\t15.85\t1.0\t0.010959\n",
    "With this in place, we can choose the columns to use, and fit a linear regression model to our data. We will set fit_intercept = False, because the daily flags essentially operate as their own day-specific intercepts:\n",
    "\n",
    "# Drop any rows with null values\n",
    "daily.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\n",
    "                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\n",
    "X = daily[column_names]\n",
    "y = daily['Total']\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "daily['predicted'] = model.predict(X)\n",
    "\n",
    "daily[['Total', 'predicted']].plot(alpha=0.5);\n",
    "\n",
    "params = pd.Series(model.coef_, index=X.columns)\n",
    "params\n",
    "Mon              504.882756\n",
    "Tue              610.233936\n",
    "Wed              592.673642\n",
    "Thu              482.358115\n",
    "Fri              177.980345\n",
    "Sat            -1103.301710\n",
    "Sun            -1133.567246\n",
    "holiday        -1187.401381\n",
    "daylight_hrs     128.851511\n",
    "PRCP            -664.834882\n",
    "dry day          547.698592\n",
    "Temp (C)          65.162791\n",
    "annual            26.942713\n",
    "dtype: float64\n",
    "\n",
    "from sklearn.utils import resample\n",
    "np.random.seed(1)\n",
    "err = np.std([model.fit(*resample(X, y)).coef_\n",
    "              for i in range(1000)], 0)\n",
    "\n",
    "print(pd.DataFrame({'effect': params.round(0),\n",
    "                    'error': err.round(0)}))\n",
    "              effect  error\n",
    "Mon            505.0   86.0\n",
    "Tue            610.0   83.0\n",
    "Wed            593.0   83.0\n",
    "Thu            482.0   85.0\n",
    "Fri            178.0   81.0\n",
    "Sat          -1103.0   80.0\n",
    "Sun          -1134.0   83.0\n",
    "holiday      -1187.0  163.0\n",
    "daylight_hrs   129.0    9.0\n",
    "PRCP          -665.0   62.0\n",
    "dry day        548.0   33.0\n",
    "Temp (C)        65.0    4.0\n",
    "annual          27.0   18.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

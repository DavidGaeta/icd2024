!pip install spacy
!pip install nltk
!pip install sklearn


import pandas as pd
import re
import spacy as spc
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer





df = pd.read_csv(r'df_mini_HS.csv')


print(df)


df['text'] = df['text'].str.lower()
df['text'] = df['text'].str.replace(r"@\S+", "", regex=True)# Eliminar menciones a usuarios
df['text'] = df['text'].str.replace(r"http[s]?\://\S+", "", regex=True)# Eliminar enlaces
df['text'] = df['text'].str.replace(r"#\S+", "", regex=True)# Eliminar hashtags
df['text'] = df['text'].str.replace(r"[0-9]", "", regex=True)# Eliminar números
df['text'] = df['text'].str.replace(r"(\(.*\))|(\[.*\])", "", regex=True)# Eliminar paréntesis y corchetes
df['text'] = df['text'].str.replace(r"\n", "", regex=True)# Eliminar caracteres de nueva línea
df['text'] = df['text'].str.replace(r"(http[s]?\://\S+)|([\[\(].*[\)\]])|([#@]\S+)|\n", "", regex=True)# Eliminar varios patrones en una sola expresión regular (enlaces, paréntesis/corchetes, hashtags, menciones y nueva línea)
df['text'] = df['text'].str.replace(r"(\.)|(,)", "", regex=True)# Eliminar puntos y comas
df['text'] = df['text'].str.replace(r"[¡!]", "", regex=True)# Eliminar signos de admiración
df['text'] = df['text'].str.replace(r"[¿?]", "", regex=True) # Eliminar signos de exclamación



print(df["text"])


dfclean = df.drop(columns = 'label')


print(dfclean)


texto_c0 = " ".join(dfclean['text'].astype(str).tolist())


print(texto_c0)


texto_c1 = re.sub(r'[\U0001F600-\U0001F64F\u263a-\U0001f645\:\*\-\"]', '', texto_c0)


print(texto_c1)


tokens = word_tokenize(texto_c1)
print(tokens)


nltk.download('stopwords')
spanish_stopwords = stopwords.words('spanish')
palabras_filtradas = [palabra for palabra in tokens if palabra not in spanish_stopwords]
print(palabras_filtradas)


#### !sudo python3 -m spacy download es


nlp = spc.load("es_core_news_sm")
lema = nlp(" ".join(palabras_filtradas))
oracion_lematizada = " ".join([token.lemma_ for token in lema])
print(oracion_lematizada)



